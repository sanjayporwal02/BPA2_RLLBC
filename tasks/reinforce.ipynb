{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RLLBC Bonus Point Assignment II Part A\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2021-06-24</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question A2 - Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: The next cell is optional, as this will _not_ run on the JupterHub. To render the CartPole environemnt, you need to set up your Jupyter environment locally (see assignment PDF). Rendering is not required for this assignment, but the visualization may help you to understand what your policy is atually learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the environment (Does not run on the JupyterHub)\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "for _ in range(10):\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Defining the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # ********************\n",
    "        # TODO Create layers\n",
    "        self.input = nn.Linear(4,128)\n",
    "        self.hidden1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(128,2)\n",
    "        self.output = nn.Softmax()\n",
    "        # ********************\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ********************\n",
    "        # TODO Implement forward pass\n",
    "      \n",
    "        x = self.input(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.output(x)\n",
    "        # ********************\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjay\\AppData\\Local\\Temp\\ipykernel_4800\\3049123097.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.output(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_output = Policy()(torch.tensor([[1.0, 2, 3, 4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "np.testing.assert_almost_equal(_test_output.detach().numpy().sum(), 1, err_msg=\"Output is not a probability distribution.\")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Action Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_action(probs):\n",
    "    \"\"\"Sample one action from the action distribution of this state.\n",
    "    \n",
    "    Args:\n",
    "        probs: action probabilities\n",
    "\n",
    "    Returns:\n",
    "        action: The sampled action\n",
    "        log_prob: Logarithm of the probability for sampling that action\n",
    "    \"\"\"\n",
    "    # TODO Implement action sampling\n",
    "    action_distribution = torch.distributions.Categorical(probs)\n",
    "    action = action_distribution.sample()\n",
    "    log_prob = action_distribution.log_prob(action)\n",
    "    \n",
    "    return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_action, _test_log_prob = sample_action(torch.tensor([1, 2, 3, 4]))\n",
    "assert _test_action in [0, 1, 2, 3], f\"Invalid action {_test_action}\"\n",
    "np.testing.assert_approx_equal(_test_log_prob, np.log((_test_action + 1) / 10))\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c) Return Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_return(rewards, gamma=0.99):\n",
    "    \"\"\"Estimate return based of observed rewards\n",
    "    \n",
    "    Args:\n",
    "        rewards: Series of observed rewards\n",
    "        gamma: discount factor\n",
    "    \"\"\"\n",
    "    # TODO Implement return estimation\n",
    "    \n",
    "    returns = [0]*(len(rewards))\n",
    "    returns[-1] = rewards[-1]\n",
    "    for i in range(len(rewards)-1):\n",
    "        returns[-2-i] = rewards[-2-i] + gamma*returns[-1-i]\n",
    "        \n",
    "    # normalization\n",
    "    mean = np.mean(returns)\n",
    "    std = np.std(returns)\n",
    "    returns = (returns - mean) / std\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "np.testing.assert_array_almost_equal(\n",
    "    estimate_return(np.ones(10), gamma=0.99),\n",
    "    [1.54572815, 1.21139962, 0.87369404, 0.53257729, 0.18801491, -0.16002789, -0.51158628, -0.86669576, -1.22539221, -1.58771185]\n",
    ")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\Sanjay\\AppData\\Local\\Temp\\ipykernel_4800\\3049123097.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.output(x)\n",
      "Mean training reward 205.18:  78%|███████▊  | 778/1000 [04:06<02:05,  1.77it/s]"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "policy = Policy()\n",
    "\n",
    "# Hyperparams\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "learn_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learn_rate)\n",
    "\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        # Run one episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            # ********************\n",
    "            # TODO Sample action for current state\n",
    "\n",
    "            probs = policy.forward(torch.tensor(state))\n",
    "            action, log_prob = sample_action(probs)\n",
    "\n",
    "            # ********************\n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "\n",
    "            # Bookkeeping\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # ********************\n",
    "        # TODO Compute loss\n",
    "\n",
    "        estimated_return = estimate_return(rewards)\n",
    "        policy_loss = 0\n",
    "        for i in range(len(log_probs)):\n",
    "            policy_loss -= log_probs[i] * estimated_return[i]\n",
    "\n",
    "        # ********************\n",
    "\n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/a2d.pt\", \"wb\") as f:\n",
    "    torch.save(policy, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\Sanjay\\anaconda3\\envs\\rllbc_bpa2\\lib\\site-packages\\gym\\core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Sanjay\\AppData\\Local\\Temp\\ipykernel_4800\\3049123097.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.output(x)\n",
      "Validating: 100%|██████████| 100/100 [00:07<00:00, 13.29it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Average reward below 487.5, got 163.5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Sanjay\\Docs\\Masters\\RWTH\\Studies\\Coding\\RLLBC\\BPA2_RLLBC\\tasks\\reinforce.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Sanjay/Docs/Masters/RWTH/Studies/Coding/RLLBC/BPA2_RLLBC/tasks/reinforce.ipynb#ch0000016?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39m_elapsed_steps\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Sanjay/Docs/Masters/RWTH/Studies/Coding/RLLBC/BPA2_RLLBC/tasks/reinforce.ipynb#ch0000016?line=14'>15</a>\u001b[0m _avg_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([_rollout(seed\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating\u001b[39m\u001b[39m\"\u001b[39m)])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Sanjay/Docs/Masters/RWTH/Studies/Coding/RLLBC/BPA2_RLLBC/tasks/reinforce.ipynb#ch0000016?line=15'>16</a>\u001b[0m \u001b[39massert\u001b[39;00m _avg_reward \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m487.5\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage reward below 487.5, got \u001b[39m\u001b[39m{\u001b[39;00m_avg_reward\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Sanjay/Docs/Masters/RWTH/Studies/Coding/RLLBC/BPA2_RLLBC/tasks/reinforce.ipynb#ch0000016?line=16'>17</a>\u001b[0m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mok (Average reward \u001b[39m\u001b[39m{\u001b[39;00m_avg_reward\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Average reward below 487.5, got 163.5"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.seed(seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "\n",
    "\n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanjay\\AppData\\Local\\Temp\\ipykernel_14628\\3049123097.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.output(x)\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the learned policy (this will not run on the JupyterHub)\n",
    "greedy = True\n",
    "\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "    if greedy:\n",
    "        action = np.argmax(probs.detach().numpy())  # Chose optimal action\n",
    "    else:\n",
    "        action = sample_action(probs)[0]  # Sample from action distribution\n",
    "    state, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rllbc_bpa2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d21eb88d67e193e3b912569b332b613e511ad6db31be33267335ca0e30f608b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
